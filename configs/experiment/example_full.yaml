# @package _global_

# to execute this experiment run:
# python run.py experiment=example_full.yaml

defaults:
  - override /trainer: null # override trainer to null so it's not loaded from main config defaults...
  - override /model: null
  - override /datamodule: null
  - override /callbacks: null
  - override /logger: null

# we override default configurations with nulls to prevent them from loading at all
# instead we define all modules and their paths directly in this config,
# so everything is stored in one place

seed: 12345

trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 0
  min_epochs: 1
  max_epochs: 10
  gradient_clip_val: 0.5
  accumulate_grad_batches: 2
  weights_summary: null
  # resume_from_checkpoint: ${work_dir}/last.ckpt

model:
  _target_: src.models.mtfm_model.MTFMModel
  text_len: 140
  embedding_size: 300
  num_api: 21569
  conv_kernel_size_tuple: (2, 3, 4, 5)
  conv_num_kernel: 256
  feature_dim: 8
  lr: 0.001
  weight_decay: 0.0005

datamodule:
  _target_: src.datamodules.mtfm_datamodule.MTFMDataModule
  data_dir: ${data_dir}
  batch_size: 64
  train_val_test_split: [ 3234, 924, 465 ]
  num_workers: 0
  pin_memory: False

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/acc"
    save_top_k: 2
    save_last: True
    mode: "max"
    dirpath: "checkpoints/"
    filename: "sample-mnist-{epoch:02d}"
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/acc"
    patience: 10
    mode: "max"

logger:
  wandb:
    tags: ["best_model", "uwu"]
    notes: "Description of this model."
  neptune:
    tags: ["best_model"]
  csv_logger:
    save_dir: "."
